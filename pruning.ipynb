{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prune the pre-trained model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d23eeffd64a04ea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from accelerate import Accelerator\n",
    "from evaluate import load\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from src.utils import calculate_sparsity\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:10:05.410845Z",
     "start_time": "2024-03-30T14:10:02.191887Z"
    }
   },
   "id": "32ab82eaf911b3a0",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set device\n",
    "accelerator = Accelerator(device_placement=True)\n",
    "device = accelerator.device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:10:05.442857Z",
     "start_time": "2024-03-30T14:10:05.411845Z"
    }
   },
   "id": "3765536794ffd899",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=100, bias=True)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained model.\n",
    "model = models.resnet50()\n",
    "model.fc = nn.Linear(model.fc.in_features, 100)  # resnet\n",
    "model.load_state_dict(torch.load('resnet_cifar100.pth'))\n",
    "\n",
    "# Fuse the model layers\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:10:05.807980Z",
     "start_time": "2024-03-30T14:10:05.443857Z"
    }
   },
   "id": "440cada178248729",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Sparsity of the model is 73.4324038028717%.'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Increase pruning ratio\n",
    "pruning_ratio = .5\n",
    "\n",
    "# Apply unstructured pruning\n",
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=pruning_ratio)\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=pruning_ratio)\n",
    "    elif isinstance(module, torch.nn.BatchNorm2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=pruning_ratio)\n",
    "        prune.l1_unstructured(module, name='bias', amount=pruning_ratio)\n",
    "\n",
    "# Apply structured pruning (channel pruning)\n",
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.ln_structured(module, name='weight', amount=pruning_ratio, n=1, dim=0)\n",
    "\n",
    "# Remove the pruning reparameterization\n",
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear) or \\\n",
    "            isinstance(module, torch.nn.BatchNorm2d):\n",
    "        prune.remove(module, 'weight')\n",
    "        if isinstance(module, torch.nn.BatchNorm2d):\n",
    "            prune.remove(module, 'bias')\n",
    "\n",
    "f'Sparsity of the model is {calculate_sparsity(model) * 100}%.'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:10:08.809301Z",
     "start_time": "2024-03-30T14:10:05.808981Z"
    }
   },
   "id": "106f1aed16103f97",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "norm = transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    norm,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    norm,\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:10:08.824304Z",
     "start_time": "2024-03-30T14:10:08.810301Z"
    }
   },
   "id": "42e5a84ca23a9ac9",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-100 dataset\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:10:09.873606Z",
     "start_time": "2024-03-30T14:10:08.825304Z"
    }
   },
   "id": "4f73415ebe7ccc25",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# Load evaluation metrics\n",
    "accuracy = load(\"accuracy\")\n",
    "f1 = load(\"f1\")\n",
    "\n",
    "model, optimizer, scheduler, train_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, scheduler, train_dataloader, test_dataloader\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:10:12.903999Z",
     "start_time": "2024-03-30T14:10:09.874608Z"
    }
   },
   "id": "f1967afbe4e639dc",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_result = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:10:12.920003Z",
     "start_time": "2024-03-30T14:10:12.905Z"
    }
   },
   "id": "436cce67a7ba03e6",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]: Train Loss: 2.4450, Test Loss: 2.1234, Train Accuracy: 0.381, Test Accuracy: 0.4480, Test F1: 0.4425, lr: 0.009999383162408303, Elapsed Time: 0:00:16\n",
      "Epoch [2/20]: Train Loss: 1.7028, Test Loss: 2.0002, Train Accuracy: 0.538, Test Accuracy: 0.4789, Test F1: 0.4771, lr: 0.009997532801828657, Elapsed Time: 0:00:16\n",
      "Epoch [3/20]: Train Loss: 1.4485, Test Loss: 1.9606, Train Accuracy: 0.601, Test Accuracy: 0.4913, Test F1: 0.4867, lr: 0.00999444937480985, Elapsed Time: 0:00:16\n",
      "Epoch [4/20]: Train Loss: 1.2841, Test Loss: 1.9638, Train Accuracy: 0.641, Test Accuracy: 0.4949, Test F1: 0.4927, lr: 0.009990133642141357, Elapsed Time: 0:00:16\n",
      "Epoch [5/20]: Train Loss: 1.1541, Test Loss: 1.9660, Train Accuracy: 0.674, Test Accuracy: 0.4987, Test F1: 0.4924, lr: 0.009984586668665639, Elapsed Time: 0:00:16\n",
      "Epoch [6/20]: Train Loss: 1.0611, Test Loss: 1.9747, Train Accuracy: 0.697, Test Accuracy: 0.4990, Test F1: 0.4978, lr: 0.009977809823015398, Elapsed Time: 0:00:16\n",
      "Epoch [7/20]: Train Loss: 0.9673, Test Loss: 2.0497, Train Accuracy: 0.723, Test Accuracy: 0.4874, Test F1: 0.4865, lr: 0.009969804777275897, Elapsed Time: 0:00:16\n",
      "Epoch [8/20]: Train Loss: 0.9045, Test Loss: 2.0434, Train Accuracy: 0.740, Test Accuracy: 0.4965, Test F1: 0.4938, lr: 0.009960573506572387, Elapsed Time: 0:00:16\n",
      "Epoch [9/20]: Train Loss: 0.8496, Test Loss: 2.0749, Train Accuracy: 0.755, Test Accuracy: 0.4914, Test F1: 0.4881, lr: 0.009950118288582784, Elapsed Time: 0:00:16\n",
      "Epoch [10/20]: Train Loss: 0.7812, Test Loss: 2.0930, Train Accuracy: 0.774, Test Accuracy: 0.4899, Test F1: 0.4869, lr: 0.009938441702975684, Elapsed Time: 0:00:16\n",
      "Epoch [11/20]: Train Loss: 0.7352, Test Loss: 2.0872, Train Accuracy: 0.786, Test Accuracy: 0.5008, Test F1: 0.4979, lr: 0.009925546630773864, Elapsed Time: 0:00:16\n",
      "Epoch [12/20]: Train Loss: 0.6804, Test Loss: 2.1085, Train Accuracy: 0.800, Test Accuracy: 0.4965, Test F1: 0.4939, lr: 0.009911436253643439, Elapsed Time: 0:00:16\n",
      "Epoch [13/20]: Train Loss: 0.6539, Test Loss: 2.1081, Train Accuracy: 0.807, Test Accuracy: 0.5002, Test F1: 0.5010, lr: 0.009896114053108824, Elapsed Time: 0:00:16\n",
      "Epoch [14/20]: Train Loss: 0.6016, Test Loss: 2.1829, Train Accuracy: 0.823, Test Accuracy: 0.4944, Test F1: 0.4911, lr: 0.009879583809693733, Elapsed Time: 0:00:16\n",
      "Epoch [15/20]: Train Loss: 0.5737, Test Loss: 2.2163, Train Accuracy: 0.832, Test Accuracy: 0.4875, Test F1: 0.4870, lr: 0.009861849601988378, Elapsed Time: 0:00:16\n",
      "Epoch [16/20]: Train Loss: 0.5507, Test Loss: 2.2135, Train Accuracy: 0.837, Test Accuracy: 0.4859, Test F1: 0.4851, lr: 0.00984291580564315, Elapsed Time: 0:00:16\n",
      "Epoch [17/20]: Train Loss: 0.5229, Test Loss: 2.2651, Train Accuracy: 0.845, Test Accuracy: 0.4825, Test F1: 0.4813, lr: 0.009822787092288985, Elapsed Time: 0:00:16\n",
      "Epoch [18/20]: Train Loss: 0.4838, Test Loss: 2.2369, Train Accuracy: 0.857, Test Accuracy: 0.4937, Test F1: 0.4932, lr: 0.00980146842838471, Elapsed Time: 0:00:16\n",
      "Epoch [19/20]: Train Loss: 0.4521, Test Loss: 2.2358, Train Accuracy: 0.865, Test Accuracy: 0.4941, Test F1: 0.4933, lr: 0.009778965073991645, Elapsed Time: 0:00:16\n",
      "Epoch [20/20]: Train Loss: 0.4343, Test Loss: 2.2326, Train Accuracy: 0.871, Test Accuracy: 0.4968, Test F1: 0.4945, lr: 0.009755282581475762, Elapsed Time: 0:00:16\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "# Fine-tune the pruned model and evaluate for sparsed model.\n",
    "for epoch in range(num_epochs):\n",
    "    tic = datetime.now()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            test_preds.extend(accelerator.gather(preds).cpu().numpy())\n",
    "            test_labels.extend(accelerator.gather(labels).cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc = accuracy.compute(references=test_labels, predictions=test_preds)[\"accuracy\"]\n",
    "    test_f1 = f1.compute(references=test_labels, predictions=test_preds, average=\"macro\")[\"f1\"]\n",
    "\n",
    "    # Update the learning rate based on validation loss\n",
    "    scheduler.step()\n",
    "\n",
    "    # Time calculation\n",
    "    toc = datetime.now()\n",
    "    elapsed_time = toc - tic\n",
    "    elapsed_time_in_hh_mm_ss = str(elapsed_time).split('.')[0]\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{num_epochs}]: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
    "        f'Train Accuracy: {correct / total:.3f}, '\n",
    "        f\"Test Accuracy: {test_acc:.4f}, Test F1: {test_f1:.4f}, \"\n",
    "        f'lr: {optimizer.param_groups[0][\"lr\"]}, '\n",
    "        f'Elapsed Time: {elapsed_time_in_hh_mm_ss}\\n'\n",
    "    )\n",
    "\n",
    "    training_result.append({\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'train_acc': correct / total,\n",
    "        'test_acc': test_acc,\n",
    "        'lr': optimizer.param_groups[0][\"lr\"]\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:15:38.664632Z",
     "start_time": "2024-03-30T14:10:12.921003Z"
    }
   },
   "id": "4e0e2a1ba75d4bf7",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Sparsity of the model is 65.28058648109436%.'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Sparsity of the model is {calculate_sparsity(model) * 100}%.'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:15:38.696143Z",
     "start_time": "2024-03-30T14:15:38.665632Z"
    }
   },
   "id": "bcc91b7c04295ee6",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   train_loss  test_loss  train_acc  test_acc        lr\n0    2.445029   2.123417    0.38120    0.4480  0.009999\n1    1.702789   2.000239    0.53774    0.4789  0.009998\n2    1.448468   1.960616    0.60126    0.4913  0.009994\n3    1.284137   1.963774    0.64080    0.4949  0.009990\n4    1.154144   1.966032    0.67362    0.4987  0.009985",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>train_loss</th>\n      <th>test_loss</th>\n      <th>train_acc</th>\n      <th>test_acc</th>\n      <th>lr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.445029</td>\n      <td>2.123417</td>\n      <td>0.38120</td>\n      <td>0.4480</td>\n      <td>0.009999</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.702789</td>\n      <td>2.000239</td>\n      <td>0.53774</td>\n      <td>0.4789</td>\n      <td>0.009998</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.448468</td>\n      <td>1.960616</td>\n      <td>0.60126</td>\n      <td>0.4913</td>\n      <td>0.009994</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.284137</td>\n      <td>1.963774</td>\n      <td>0.64080</td>\n      <td>0.4949</td>\n      <td>0.009990</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.154144</td>\n      <td>1.966032</td>\n      <td>0.67362</td>\n      <td>0.4987</td>\n      <td>0.009985</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index is epoch number.\n",
    "tr = pd.DataFrame(training_result, columns=['train_loss', 'test_loss', 'train_acc', 'test_acc', 'lr'])\n",
    "tr.to_csv('pruned_model_result.csv')\n",
    "tr.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:15:38.712147Z",
     "start_time": "2024-03-30T14:15:38.697143Z"
    }
   },
   "id": "3bcebd7c1932310e",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model size: 90.76 MB\n"
     ]
    }
   ],
   "source": [
    "# Save the pruned model to disk\n",
    "torch.save(model.state_dict(), \"pruned_model.pth\")\n",
    "\n",
    "# Get the size of the saved model file\n",
    "model_size = os.path.getsize(\"pruned_model.pth\") / (1024 * 1024)  # Size in MB\n",
    "print(f\"Pruned model size: {model_size:.2f} MB\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T14:15:38.838402Z",
     "start_time": "2024-03-30T14:15:38.713147Z"
    }
   },
   "id": "7099180f1ffa376",
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
